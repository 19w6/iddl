{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from utils import progress_bar\n",
    "from utils import format_time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./data', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=100, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./data', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=100, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./data', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "# print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "def random_weight(shape): \n",
    "    \"\"\"\n",
    "    Initialization\n",
    "    The BIBD part can be added in this function.\n",
    "    Create random Tensors for weights; setting requires_grad=True means that we\n",
    "    want to compute gradients for these Tensors during the backward pass.\n",
    "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
    "    # randn is standard normal distribution generator. \n",
    "    \n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
    "    w.requires_grad = True\n",
    "    \n",
    "    return w\n",
    "\n",
    "def zero_weight(shape):\n",
    "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "def check_accuracy_part(loader, model_fn, params, num_exit):\n",
    "    \"\"\"\n",
    "    Check the accuracy of a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - loader: A DataLoader for the data split we want to check\n",
    "    - model_fn: A function that performs the forward pass of the model,\n",
    "      with the signature scores = model_fn(x, params)\n",
    "    - params: List of PyTorch Tensors giving parameters of the model\n",
    "    - num_exit: The number of exits in the net.\n",
    "    \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_exit += 1\n",
    "    split = 'val' if loader.dataset.train else 'test'\n",
    "    print('Checking accuracy on the %s set' % split)\n",
    "    num_samples = 0\n",
    "    num_correct = np.zeros(num_exit)\n",
    "    \n",
    "    with torch.no_grad(): # this is testing part step\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.int64)\n",
    "            outputs = model_fn(x, params) # if there are many exits, the scores will be a vector\n",
    "            \n",
    "            for i in range(num_exit):\n",
    "                _, preds = outputs[i].max(1)\n",
    "                num_correct[i] += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        \n",
    "        acc = num_correct / float(num_samples)\n",
    "        \n",
    "        msg = ''\n",
    "        for i in range(num_exit):\n",
    "            msg = msg + '| Ex%d Acc: %.2f%%' % (i, 100. * num_correct[i] / num_samples)\n",
    "        \n",
    "        print(msg)\n",
    "        \n",
    "def train_part(model_fn, params, learning_rate, num_exit):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model.\n",
    "      It should have the signature scores = model_fn(x, params) where x is a\n",
    "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
    "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
    "      scores for the elements in x.\n",
    "    - params: List of PyTorch Tensors giving weights for the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
    "    - num_exit: The number of exits in the net. \n",
    "    \n",
    "    Returns: Nothing\n",
    "    \"\"\"\n",
    "    num_exit += 1\n",
    "    train_loss = 0\n",
    "    num_samples = 0\n",
    "    num_correct = np.zeros(num_exit)\n",
    "    \n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "        # Move the data to the proper device (GPU or CPU)\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass: compute scores and loss\n",
    "        outputs = model_fn(x, params) # if there are many exits, the scores will be a vector\n",
    "        \n",
    "        # In this case, we determine the scores by 0.9*out0 + 0.09*out1 + 0.009*out2 + ...\n",
    "        mask = np.array([9 * 10 ** (-i - 1) for i in range(num_exit)])\n",
    "        scores = outputs.dot(mask)\n",
    "        \n",
    "        loss = F.cross_entropy(scores, y) # this scores should be a weighted? or only the last exit?\n",
    "\n",
    "        # Backward pass: PyTorch figures out which Tensors in the computational\n",
    "        # graph has requires_grad=True and uses backpropagation to compute the\n",
    "        # gradient of the loss with respect to these Tensors, and stores the\n",
    "        # gradients in the .grad attribute of each Tensor.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters. We don't want to backpropagate through the\n",
    "        # parameter updates, so we scope the updates under a torch.no_grad()\n",
    "        # context manager to prevent a computational graph from being built.\n",
    "        with torch.no_grad():\n",
    "            for w in params:\n",
    "                #print (w.shape)\n",
    "                w -= learning_rate * w.grad\n",
    "\n",
    "                # Manually zero the gradients after running the backward pass\n",
    "                w.grad.zero_()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        for i in range(num_exit):\n",
    "                _, preds = outputs[i].max(1)\n",
    "                num_correct[i] += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "        acc = num_correct / float(num_samples)\n",
    "        \n",
    "        msg = 'Loss: %.2f' % (train_loss / (t + 1))\n",
    "        \n",
    "        for i in range(num_exit):\n",
    "            msg = msg + '| Ex%d Acc: %.2f%%' % (i, 100. * num_correct[i] / num_samples)\n",
    "        msg += ' .'\n",
    "        \n",
    "        progress_bar(t, len(loader_train), msg)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the model(forward) of the Naive CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet_3exit(x, params):\n",
    "    \"\"\"\n",
    "    Performs the forward pass of a exit convolutional network with the\n",
    "    architecture defined above.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images\n",
    "    - params: A list of PyTorch Tensors giving the weights and biases for the\n",
    "      network;\n",
    "    \n",
    "    Returns:\n",
    "    - scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
    "    \"\"\"\n",
    "    conv1, conv2, conv3, conv4, conv5, fc = params\n",
    "    conv1 = F.relu(F.conv2d(x, conv1[0], conv1[1], stride = 2))\n",
    "    maxp = \n",
    "    # print (\"x.shape\", x.shape) [32, 3, 32, 32]\n",
    "    l1_output = F.relu ( F.conv2d(x, conv_w1, conv_b1, padding = 2) )\n",
    "    # print (\"l1_output.shape\", l1_output.shape) [32, 32, 32, 32]\n",
    "    l2_output = F.relu ( F.conv2d(l1_output, conv_w2, conv_b2, padding = 1) )\n",
    "    # print (\"l2_output.shape\", l2_output.shape) [32, 16, 32, 32]\n",
    "    scores = flatten(l2_output).mm(fc_w) + fc_b\n",
    "    # print (\"scores.shape\", scores.shape) [32, 10]\n",
    "    \n",
    "    \n",
    "    exit = flatten(l1_output).mm(e1_w) + e1_b\n",
    "    \n",
    "    return np.array([scores, exit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the NaiveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 42ms | Tot: 20s757ms | Loss: 1.76| Ex0 Acc: 37.74%| Ex1 Acc: 18.77% 490/490 /490 \n",
      "Checking accuracy on the val set\n",
      "| Ex0 Acc: 44.80%| Ex1 Acc: 20.20%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "\n",
    "'''\n",
    "# initialization\n",
    "params = []\n",
    "conv1_w = random_weight((64, 3, 7, 7)) # s = 2\n",
    "conv1_b = zero_weight((64,))\n",
    "\n",
    "conv1 = [conv1_w, conv1_b]\n",
    "conv2 = [0 for i in range(8)]\n",
    "conv3 = [0 for i in range(8)]\n",
    "conv4 = [0 for i in range(8)]\n",
    "conv5 = [0 for i in range(8)]\n",
    "\n",
    "for i in range(4):\n",
    "    conv2[2*i] = random_weight((64, 64, 3, 3))\n",
    "    conv2[2*i + 1] = zero_weight((64,))\n",
    "    conv3[2*i] = random_weight((128, 64, 3, 3))\n",
    "    conv3[2*i + 1] = zero_weight((128,))\n",
    "    conv4[2*i] = random_weight((256, 128, 3, 3))\n",
    "    conv4[2*i + 1] = zero_weight((256,))\n",
    "    conv5[2*i] = random_weight((512, 256, 3, 3))\n",
    "    conv5[2*i + 1] = zero_weight((512,))\n",
    "\n",
    "fc_w = random_weight((512, 10))\n",
    "fc_b = zero_weight((10,))\n",
    "fc = [fc_w, fc_b]\n",
    "\n",
    "params = [conv1, conv2, conv3, conv4, conv5, fc] # even for weight, odd for bias\n",
    "'''\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_part(NaiveNet, params, learning_rate, 1)\n",
    "    check_accuracy_part(loader_val, NaiveNet, params, 1)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
