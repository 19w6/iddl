{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# For auto reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset with MLP\n",
    "\n",
    "[reference](https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.3.1  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next we'll load the MNIST data. First time we may have to download the data, which can take a while.\n",
    "\n",
    "Note that we are here using the MNIST test data for *validation*, instead of for testing the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. The first element of training data (`X_train`) is a 4th-order tensor of size (`batch_size`, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. `y_train` is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x108 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize = 1.5\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28, 28), cmap=\"gray_r\")\n",
    "    plt.title('Class: {}'.format(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP network definition\n",
    "\n",
    "Let's define the network as a Python class.  We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass.\n",
    "\n",
    "Finally, we define an optimizer to update the model parameters based on the computed gradients.  We select *stochastic gradient descent (with momentum)* as the optimization algorithm, and set *learning rate* to 0.01.  Note that there are [several different options](http://pytorch.org/docs/optim.html#algorithms) for the optimizer in PyTorch that we could use instead of *SGD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=49, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (bibd2): BibdLinear()\n",
      "  (fc3): Linear(in_features=56, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../bibd')\n",
    "from bibd_layer import BibdLinear\n",
    "\n",
    "q = 7\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, q*q)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.bibd2 = BibdLinear(q*q, q*(q+1), number_of_block=q)\n",
    "        self.fc3 = nn.Linear(q*(q+1), 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.bibd2(x))\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Let's now define functions to `train()` and `validate()` the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function.  An *epoch* means one pass through the whole training data. After each epoch, we evaluate the model using `validate()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303854\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.152394\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.674864\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.057678\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.896833\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.629717\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.698743\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.569789\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.345635\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.396329\n",
      "\n",
      "Validation set: Average loss: 0.3766, Accuracy: 8955/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.558998\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.340414\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.255624\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.513889\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.445894\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.495725\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.286029\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.897588\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.523452\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.345096\n",
      "\n",
      "Validation set: Average loss: 0.2812, Accuracy: 9213/10000 (92%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.445127\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.430366\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.236646\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.112106\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.520853\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.180374\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.286437\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.234263\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.261022\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.124232\n",
      "\n",
      "Validation set: Average loss: 0.2423, Accuracy: 9292/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.358735\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.073213\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.564088\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.389858\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.140818\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.263115\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.243867\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.451744\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.352502\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.185917\n",
      "\n",
      "Validation set: Average loss: 0.2135, Accuracy: 9377/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.221991\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.169217\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.322655\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.048176\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.489013\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.556377\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.179447\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.289190\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.237229\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.191353\n",
      "\n",
      "Validation set: Average loss: 0.1958, Accuracy: 9413/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.372535\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.467578\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.165823\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.517841\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.261172\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.316920\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.222352\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.067884\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.419142\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.331976\n",
      "\n",
      "Validation set: Average loss: 0.1783, Accuracy: 9458/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.198326\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.440526\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.134218\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.233262\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.344962\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.065949\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.324440\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.318040\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.088742\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.066172\n",
      "\n",
      "Validation set: Average loss: 0.1678, Accuracy: 9491/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.057239\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.060370\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.321217\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.200691\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.068471\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.042083\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.068242\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.363412\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.165243\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.244430\n",
      "\n",
      "Validation set: Average loss: 0.1587, Accuracy: 9505/10000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.361186\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.163312\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.116530\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.467188\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.094498\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.101301\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.102607\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.428936\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.227109\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.261481\n",
      "\n",
      "Validation set: Average loss: 0.1452, Accuracy: 9558/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.106935\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.327110\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.090054\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.146871\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.072977\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.179618\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.149566\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.153254\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.161053\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.468707\n",
      "\n",
      "Validation set: Average loss: 0.1393, Accuracy: 9572/10000 (96%)\n",
      "\n",
      "CPU times: user 1min 38s, sys: 13.3 s, total: 1min 52s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Register FLOP counter for module Linear(in_features=784, out_features=49, bias=True)\n",
      "Register FLOP counter for module Dropout(p=0.2, inplace=False)\n",
      "Register FLOP counter for module BibdLinear()\n",
      "Register FLOP counter for module Linear(in_features=56, out_features=10, bias=True)\n",
      "Params(M): 0.04, FLOPs(M): 0.08\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "import sys\n",
    "sys.path.append('../bibd')\n",
    "from bibd_layer import BibdLinear\n",
    "\n",
    "\n",
    "def count_model(model_to_count, x, y):\n",
    "    total_ops = np.sum(model_to_count.mask.cpu().numpy())\n",
    "\n",
    "    model_to_count.total_ops += torch.Tensor([int(total_ops)])\n",
    "\n",
    "input = torch.randn(1, 28, 28).to(device)\n",
    "flops, params = profile(Net().to(device), inputs=(input, ), custom_ops={BibdLinear: count_model})\n",
    "print('Params(M): %.2f, FLOPs(M): %.2f' % (params / (1000 ** 2), flops / (1000 ** 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
