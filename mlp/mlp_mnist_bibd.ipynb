{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# For auto reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset with MLP\n",
    "\n",
    "[reference](https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.3.1  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next we'll load the MNIST data. First time we may have to download the data, which can take a while.\n",
    "\n",
    "Note that we are here using the MNIST test data for *validation*, instead of for testing the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. The first element of training data (`X_train`) is a 4th-order tensor of size (`batch_size`, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. `y_train` is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x108 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize = 1.5\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28, 28), cmap=\"gray_r\")\n",
    "    plt.title('Class: {}'.format(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP network definition\n",
    "\n",
    "Let's define the network as a Python class.  We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass.\n",
    "\n",
    "Finally, we define an optimizer to update the model parameters based on the computed gradients.  We select *stochastic gradient descent (with momentum)* as the optimization algorithm, and set *learning rate* to 0.01.  Note that there are [several different options](http://pytorch.org/docs/optim.html#algorithms) for the optimizer in PyTorch that we could use instead of *SGD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=49, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (bibd2): BibdLinear()\n",
      "  (fc3): Linear(in_features=56, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../bibd')\n",
    "from bibd_layer import BibdLinear\n",
    "\n",
    "r = 7\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, r*r)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.bibd2 = BibdLinear(r*r, r*(r+1), number_of_block=r)\n",
    "        self.fc3 = nn.Linear(r*(r+1), 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.bibd2(x))\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Let's now define functions to `train()` and `validate()` the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function.  An *epoch* means one pass through the whole training data. After each epoch, we evaluate the model using `validate()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296666\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.156538\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.664729\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.225352\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.929848\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.145967\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.521660\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.989820\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.642475\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.550099\n",
      "\n",
      "Validation set: Average loss: 0.3929, Accuracy: 8911/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.519966\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.819845\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.702104\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.394798\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.312865\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.339406\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.682509\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.746598\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.382740\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.474658\n",
      "\n",
      "Validation set: Average loss: 0.2858, Accuracy: 9192/10000 (92%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.337513\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.125904\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.353088\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.341111\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.471367\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.171157\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.330693\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.210600\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.461299\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.306115\n",
      "\n",
      "Validation set: Average loss: 0.2401, Accuracy: 9306/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.297333\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.306152\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.354787\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.216223\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.324999\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.221016\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.224067\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.446305\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.143062\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.419642\n",
      "\n",
      "Validation set: Average loss: 0.2084, Accuracy: 9402/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.122546\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.103935\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.222511\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.569930\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.473810\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.233609\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.172844\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.117989\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.277558\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.509804\n",
      "\n",
      "Validation set: Average loss: 0.1887, Accuracy: 9453/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.239033\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.235368\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.466957\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.395691\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.176085\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.243744\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.185071\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.162526\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.411096\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.193182\n",
      "\n",
      "Validation set: Average loss: 0.1733, Accuracy: 9503/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.161104\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.198585\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.235633\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.134025\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.228393\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.092480\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.193214\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.108442\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.109480\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.278957\n",
      "\n",
      "Validation set: Average loss: 0.1619, Accuracy: 9531/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.165783\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.236934\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.225471\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.193129\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.227907\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.859885\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.225373\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.242194\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.057446\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.242130\n",
      "\n",
      "Validation set: Average loss: 0.1487, Accuracy: 9552/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.234062\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.167722\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.188483\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.125468\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.315401\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.237485\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.081804\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.124023\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.108600\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.357147\n",
      "\n",
      "Validation set: Average loss: 0.1436, Accuracy: 9577/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.037117\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.135785\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.158083\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.046485\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.258653\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.472495\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.084320\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.203966\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.236176\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.067187\n",
      "\n",
      "Validation set: Average loss: 0.1382, Accuracy: 9600/10000 (96%)\n",
      "\n",
      "CPU times: user 1min 35s, sys: 14.2 s, total: 1min 49s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
