{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# For auto reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset with MLP\n",
    "\n",
    "[reference](https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.3.1  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next we'll load the MNIST data. First time we may have to download the data, which can take a while.\n",
    "\n",
    "Note that we are here using the MNIST test data for *validation*, instead of for testing the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. The first element of training data (`X_train`) is a 4th-order tensor of size (`batch_size`, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. `y_train` is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x108 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize = 1.5\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28, 28), cmap=\"gray_r\")\n",
    "    plt.title('Class: {}'.format(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP network definition\n",
    "\n",
    "Let's define the network as a Python class.  We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass.\n",
    "\n",
    "Finally, we define an optimizer to update the model parameters based on the computed gradients.  We select *stochastic gradient descent (with momentum)* as the optimization algorithm, and set *learning rate* to 0.01.  Note that there are [several different options](http://pytorch.org/docs/optim.html#algorithms) for the optimizer in PyTorch that we could use instead of *SGD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "Current device: 0\n",
      "Current device name: Tesla K80\n",
      "Cuda device count: 1\n",
      "Cuda is available. Running the model using DataParallel...\n",
      "DataParallel(\n",
      "  (module): Net(\n",
      "    (fc1): Linear(in_features=784, out_features=49, bias=True)\n",
      "    (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "    (fc2): Linear(in_features=49, out_features=56, bias=True)\n",
      "    (fc3): Linear(in_features=56, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 49)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(49, 56)\n",
    "#         self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(56, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.fc2_drop(x)\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print('device = {}'.format(device))\n",
    "print('Current device: {}'.format(torch.cuda.current_device()))\n",
    "print('Current device name: {}'.format(torch.cuda.get_device_name(torch.cuda.current_device())))\n",
    "print('Cuda device count: {}'.format(torch.cuda.device_count()))\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    print('Cuda is available. Running the model using DataParallel...')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Let's now define functions to `train()` and `validate()` the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function.  An *epoch* means one pass through the whole training data. After each epoch, we evaluate the model using `validate()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312514\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.037329\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.157947\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.906247\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.630306\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.703437\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.730844\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.333246\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.418823\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.681032\n",
      "\n",
      "Validation set: Average loss: 0.3321, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.337443\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.645116\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.385086\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.284744\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.842876\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.210902\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.265622\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.390676\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.201814\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.382466\n",
      "\n",
      "Validation set: Average loss: 0.2450, Accuracy: 9289/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.256355\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.256866\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.222227\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.266814\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.334761\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.202023\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.173366\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.265773\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.327441\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.102717\n",
      "\n",
      "Validation set: Average loss: 0.2033, Accuracy: 9424/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.188389\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.297726\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.198582\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.486699\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.147378\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.277421\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.229999\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.054549\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.064747\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.048446\n",
      "\n",
      "Validation set: Average loss: 0.1785, Accuracy: 9459/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.234260\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.317583\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.274561\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.366997\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.213556\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.377097\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.383285\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.609596\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.135474\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.176167\n",
      "\n",
      "Validation set: Average loss: 0.1562, Accuracy: 9535/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.151396\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.353653\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.046658\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.282258\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.263319\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.266812\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.271613\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.198557\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.260606\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.281587\n",
      "\n",
      "Validation set: Average loss: 0.1459, Accuracy: 9563/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.223244\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.269437\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.044096\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.282920\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.244692\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.179011\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.250873\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.165293\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.151071\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.148874\n",
      "\n",
      "Validation set: Average loss: 0.1348, Accuracy: 9598/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.350873\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.363304\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.162695\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.098671\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.202294\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.405659\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.048895\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.085015\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.077190\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.119604\n",
      "\n",
      "Validation set: Average loss: 0.1233, Accuracy: 9622/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.124937\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.313454\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.264739\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.293591\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.224572\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.132056\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.128316\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.228768\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.217180\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.147307\n",
      "\n",
      "Validation set: Average loss: 0.1168, Accuracy: 9640/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.012521\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.077756\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.123189\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.098336\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.353188\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.053225\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.327071\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.055347\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.481659\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.269753\n",
      "\n",
      "Validation set: Average loss: 0.1167, Accuracy: 9647/10000 (96%)\n",
      "\n",
      "CPU times: user 1min 40s, sys: 13.8 s, total: 1min 53s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Register FLOP counter for module Linear(in_features=784, out_features=49, bias=True)\n",
      "Register FLOP counter for module Dropout(p=0.2, inplace=False)\n",
      "Register FLOP counter for module Linear(in_features=49, out_features=56, bias=True)\n",
      "Register FLOP counter for module Linear(in_features=56, out_features=10, bias=True)\n",
      "Params(M): 0.04, FLOPs(M): 0.08\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "\n",
    "input = torch.randn(1, 28, 28).to(device)\n",
    "flops, params = profile(Net().to(device), inputs=(input, ))\n",
    "print('Params(M): %.2f, FLOPs(M): %.2f' % (params / (1000 ** 2), flops / (1000 ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
