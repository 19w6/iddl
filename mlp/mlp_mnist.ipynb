{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# For auto reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset with MLP\n",
    "\n",
    "[reference](https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.3.1  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next we'll load the MNIST data. First time we may have to download the data, which can take a while.\n",
    "\n",
    "Note that we are here using the MNIST test data for *validation*, instead of for testing the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. The first element of training data (`X_train`) is a 4th-order tensor of size (`batch_size`, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. `y_train` is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAABlCAYAAACoc7mxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfC0lEQVR4nO3dedxUVR3H8c9RWXJFXEkTMyUTEnuBlooKZiCJmCKyWKaWqBm4Jy4oipIpGeKKhCEYbiihqLHFpphImGKJuKFiUAiIIKIs0x/znDNnmPvMM3O5M3PvPN/368XL45ntzO+5M89z7/md3zGpVAoREREREREpzjaVHoCIiIiIiEgS6WRKREREREQkBJ1MiYiIiIiIhKCTKRERERERkRB0MiUiIiIiIhKCTqZERERERERCKPvJlDFmoDHm4XK/bjVTTKOnmEZPMY2eYho9xbQ0FNfoKabRU0yjVx9iWpKTKWNMb2PMPGPMWmPMUmPM88aYdqV4rWIZYwYZYxYYYzYaYwZWejyFUkyjF/OYHmaMmW2MWW2MWWKMub7SYypEzGN6lDFmrjFmjTHm9biMqy4xj+n+xpjpxph1xpiFxpgTKj2mQsQ8pon87EO842oZY44zxqSMMTdXeiyFiHNM9fmPfFz71YzJ/5cyxlxe6bHVJa4xhfIcp5GfTBljLgOGAoOBvYD9gHuBU6J+rZDeAX4DPFvpgRRKMY1eAmI6FpgFNAWOAy40xnSt7JDyi3NMjTFNgaeB24EmwG3AM8aYXSs6sDrEOaY1HgFeBXYDrgXGGWP2qOyQ8ktATBP32YdExBVjTAPgTuDlSo+lEAmIqT7/EUqlUh+mUqkd7T/gu8Bm4MkKDy2vOMe0RumP01QqFdk/YBdgLdA9z30GAg97//8EsAxYTfoXSEvvth8D/wbWAB8DV9T07w5MBD4FVgKzgW2KHOvDwMAo338p/imm9TOmwDrgkC1e/+pKxy6pMQW6AP/aom8R8ItKxy7BMW0BfAns5PXNBi6odOySGtOaxybqs5+UuNY8vj/pCymjgJsrHbckx1Sf/9L+PVXzPDcA0ysdtyTHtFzHadQzU0cCjYHxRTzmeeAgYE9gPvBn77aRwPmpVGonoBXwt5r+y4ElwB6kz4KvAVIAxph7jTH3bsV7iBvFNHpJiOlQ4CxjTANjzLdrxjy1iPGWW9xjamr+bdnXqojxllvcY9oSeC+VSq3x+l6r6Y+ruMcUkvfZhwTE1RjTHDgXuKmIMVZS3GOqz3/p/546C3ioiLFWQtxjWpbjdLson4z0FNonqVRqY6EPSKVSD9q2Sa+3WWWM2SWVSq0GNgCHGGNeS6VSq4BVNXfdADQDmqdSqXdIn2Xa5/vV1r+NWFFMo5eEmE4ERgNXANsCN6VSqVcKHW8FxD2mc4CvG2N6AeOA3sC3gO0LHW8FxD2mO5K+suhbDexT6HgrIO4xheR99iEZcR0GDEilUmuN2fK6SizFPab6/Jfw7yljzDGkTxrGFTrWCol7TMtynEY9M7UC2N0YU9BJmjFmW2PMrcaYd40xnwGLa27avea/3UhP+X1gjJlpjDmypv920ut0Jhtj3jPG9I/uLcSOYhq9WMe0Zn3PX0lfQW0MfAPoZIyJ80ltrGOaSqVWkM7fvgz4L3Ai6av9Swp5fIXEOqakUzt23qJvZ9LpGXEV65gm9LMP8Y/ryaTTfB4r8P3EQaxjij7/UNq/p34OPJlKpdaGeGw5xT2m5TlOo8wZJJM7eXqe+wykJncS+BnwJvBN0ik3TUhP2x24xWMaAJcCHwU8X0vgf8APixxr0tb3KKb1JKZAW2DVFn2XABMrHbukxjTgsdsBHwCdKh27pMaUdC76erJz0WeRjDUTcY1p4j77CYnrUOAz0us0lgFf1Ix3QqVjl+CY6vMfcUy9x3yN9OzJ8ZWOWdJjWq7jNNKZqVR6iu564B5jzE+MMdubdN53Z2PMbQEP2Yn0wrAVpNNtBtsbjDENjTFn1kz9bSD9Rbip5rYuxpgDjTHG699UyBhrxtOY9KzcdsaYxsaYbcO/69JSTKOXgJguSj/c9DbGbGOM2RvoQTrPN5YSEFOMMd+rGdPOwBBgSSqVmhT+XZdW3GOaSqUWAf8Ebqj5zJ8KHEqMK0/FPaYk8LMPiYjrANJ/VB1W8+9pYARwTsi3XHJxj6k+/6X5PVXjVNKFFqaHeJtlFfeYlu04LdGZ6pnAPOBz0leBngWOCjhD3RGYQHq67QPSi+1SwIFAQ9LpDqtIB+4VoF3N4y4lPTX4Oek0nQHea98P3J9nbKNqXsP/d3YlzugVU8U0z9iOr3mu1TVjGwFsX+mYJTymj9TEczXwGLBnpeNVBTHdH5hB+kr/W8AJlY5XFcQ0kZ/9uMd1i3GOIubV/JIQU33+S3OcApOAQZWOU7XEtBzHqal5IRERERERESlC5Jv2ioiIiIiI1Ac6mRIREREREQlBJ1MiIiIiIiIh6GRKREREREQkBJ1MiYiIiIiIhFDXjsUq9Vc7E/JximntFNPohY0pKK756FiNnmIaPcU0eopp9BTT6Cmm0as1ppqZEhERERERCUEnUyIiIiIiIiHoZEpERERERCQEnUyJiIiIiIiEoJMpERERERGREHQyJSIiIiIiEoJOpkRERERERELQyZSIiIiIiEgIOpkSEREREREJYbtKD6AuK1asAKBbt26uL5XKbNA8fvx4AJo2bVregdUDCxYscO1OnToBsGzZMte3efPmso9JRERECuP/nh48eDAAI0aMcH3HHHOMax9yyCEA9O/f3/Vts42uuYvURZ8SERERERGREHQyJSIiIiIiEoLxU+YC5L2xHGwa3+mnn+76/DH37NkTgLFjx5Z3YGBCPq7iMa3Ll19+CUCXLl1c39/+9rec+23atCnql67amFqjR4927dtuuw3IpFYAXHTRRQAcd9xxUb1k2JhCguJaAVV/rFZAYmO6bt06ANavX+/6nnzySdd+9913AXjjjTdc37PPPgvAlVde6fo6duwIZKdeNWrUaGuGltiY1sXG/Mgjj3R9gwYNAqBr166lfOlExXT+/Pmu3aZNm5zbjz32WNeeNWsWALNnz3Z9rVu3znnMDjvsAESaApiomCaEYhq9WmOqmSkREREREZEQYl+AYujQoXlv/89//lOmkdQfw4YNA4Jno3r16lXu4STC8uXLAZg3b57re+qpp1x75syZAHz00Ueu76uvvgLgzTffdH1f//rXgUhnpqrCww8/7Np+XO3Mtc8WS3niiSdc30477VTC0SXL6tWrgcwMNMC9997r2sYUdkFz1113BeCCCy5wfQ0bNoxiiInQt29f17afb3/mqS42zkOGDHF9tn3PPfe4vgsvvHCrxpkEa9asAeChhx5yfX369AFqP6aWLFkCZBdK6t27NwDPPPOM6+vQoUO0g02IjRs3AtmxCGJno3z+zGiQu+66C4Bf/epXrk+FKiSMxYsXA5nPc23atWtXhtGEp6NfREREREQkBJ1MiYiIiIiIhBD7NL+62LSpf//7367PX9Avxbv55ptrve26664r40ji4+OPP3btMWPGALDHHnu4vgceeADITvPz2aIphaZQ1Wd+KuRvfvMbAB599NHA+zZv3hyAxo0bu75p06YBMGDAANdXV7pwtfPf/x/+8AcgO63CL+pT6DFqH/OXv/zF9dmUtO7du4cfbMw9//zzAAwfPtz12ZQqX4sWLVy7bdu2AJx22mmub+LEiQCMGjUq57H2+wSqN83PP+YmTJgAQL9+/VzfwoULAbj77rsDH//WW2/l9NmiFK+99prrq69pfnfccQcAAwcOjPy5bYrr9ttv7/rOPffcyF8nrvxj1372/RTfO++807U//PBDAKZPn+76bBqvnyZZn/i/422hM/9v+CArV6507UJ/R9njs0GDBsUOsWiamRIREREREQkhljNT/iyAfzYaxC5ee/31112fZqaKZxdQA3z22WdA9tn/GWecAcDBBx9c3oFVgB8LW+Dg6aefdn0ffPABEO5qvmTziyDYGQ47GwWZq3q2ZDTAVVdd5dqHH344kF1gwl4hve+++1yfnRHwywBXK39B+S233ALAjBkzXF/QLIpf8OSUU04BsmNuC/3YGQT/dfzPi726uMsuu7g+/3mqweDBg4HgONqZUoAXXnjBtXfffXcg+3ebLeNdX/nFY84666yc220BGX9mxcYRYO7cuaUbXEJt2LDBtf3y5oXae++9gcwWHZD5G8yfIbSv4xdPsYV/9tlnn6JfNy78z7R9j4sWLXJ948aNA2DOnDmuL6hQV13ssV8fZqZeeukl17a/K/xCM0EzzEFswSMo/O+ts88+G4DzzjvP9flbKURJM1MiIiIiIiIh6GRKREREREQkhFim+QWljUj0VqxY4dr+Yn07hepPpZ500knlG1iF2OPu+OOPL/qxv/3tbwE49dRTXV+hKZF+ilV9KZRg99b64Q9/6PqWLl2ac78bbrgBKG4RtV3E/qc//cn12QW/1Zbm56eg2HQTv2BBUDrE97//fSCTrgpw9NFHu/Yll1yS8xibOn3CCSfk3OYXoLCPPfHEE12fLWhzzTXX5HsrsWaLTgC88sorObe3adMGgP79+7s+PyXNmjp1qmv7RRLqI5syVZtGjRoB2SmjvqCfQ31ni8tApsBJkH333de1zz//fNe2e8YFHbvbbruta9v0Pn+PRFucauTIka4vjntPvf3220CmQAdA69atgez93fLtGee/r5133hmAli1buj6/AM+mTZsAuPLKK7dm2Iljl4v8+Mc/dn12j8NyscV9/JTi5557Dog+3S9+R7qIiIiIiEgCxHJmyl/sL6XjL5x88cUXc27v1q2ba//kJz8py5jKzRYwgexZpWLZK3lBu8n79txzT9fu3LkzkH01sZr5V/rs1Sp/NsrOeowYMcL1hVnM3KpVq6z/VqNPP/0UgB49erg+W4THn42yC2979+7t+uwsk3/s20IeYfjfDS+//DIAt912m+t7/PHHgWTPTPlljb/66quc288880wg+zvT9/777wOZ4hW+hg0buraN1RFHHBF+sDEXVNTH8mNhj5fayhr7syJbqm9FqJYvXw5kF93Jx5+1D5ptDuIXpbBX/D/55JOcvrvuusv17bjjjgU9dznZ76j777+/oPs3a9bMtXv27Alkb3HQrl27vI/3C9FUu/Xr17v27bffDkQ3G+UX9wnKuLCvY383Bt0GmWJqkyZNcn1RfF9oZkpERERERCQEnUyJiIiIiIiEEMs0P3/6rS42bcqfApT87J4xde1zYqdDIXsfn2qyxx57uPYjjzwCZC+YLJS/j0E+NrUP4MEHHyz6dZLIpuP4cV2zZg0AJ598susbPXo0AE2aNNmq19tuu/TXmp+2uWDBAiDZe4P5KRQ2NdTfX+9rX/saAD/96U9dn025CUqV8o/9qNjvFL+4Ql2pr0kwZcqUnD4/Jc1PgbI2b97s2ueccw6QWfwOmQX9//3vf11fbcUWqslNN90EwLp163Ju89OnbDEEn39c2dS2IG3btnVtm866//77FzvUxLBpe37qbpD27dsDcOihhxb9Gn78dthhByA7zS8pbKqe//l87733gEzRBIDDDjsMyE5jtsUmJJhfyOz3v/996Oex+51dfPHFrs/fXzLIrbfeCtSdTm73+rNpqZCdlh6WZqZERERERERCiOXMVDG+/e1vA6Xb1bga2fLFtZX+tGVTw1y9Shp7hQ2gY8eOANx555059xs+fLhrb025/muvvTb0Y5PE39Xclj/fa6+9XN/YsWOB7HK7djd4e9vW8me4bPluv/Ttr3/960hep1z8BdO23LjPHltXX3112ca0JVsG2L/CG6aISBLY9wowZ84cIHubA/9nFDQ7Z39O9WE2yi925H83WHYmyS+MEOTZZ591bX+mdktXXHGFa9vZQH8bhmK2WogrvxBKviIHfhGIW265BcguhFSoadOmubY/m2rZGZzGjRsX/dzlZDMXzjrrrLK8XtBn/8ADDyzLa5fb5MmTI3meMWPGANmf2brYIhK2GBDAjBkzgMxslM8fq2amREREREREKkQnUyIiIiIiIiEkPs1PCrNixQrXtvss1LYA3055tmjRovQDixEbj6D0L38/H5sqEZQOWBu/8EE1++KLL4DMIl/ILNp94IEHXF/QvkZTp07Neg7IFFUoxvz584HgqXt/QeuGDRsAuPTSS4t+jXJauXIlELwvyrnnnuval112WdnGVBu7qHtrf4Zx07Rp05w+P83v1VdfBbLTd0aOHJnzmGOOOca1K5mOWW5+mnRQep7dy8f/nrTH/eeff+76Ck3H8ReX2++fatsr0f+d/swzz9R6vxtvvNG1jzrqqNCv5+/FY1NT/Z/lddddB2TS6CQtqEhH69atKzCS0rF7x9lCHrXp3r07AH379s17vzBLTLp27Zr1X8gUuApK83v33Xdd2y4D2JrvCM1MiYiIiIiIhBDLSwj+1amgK/p13S65/KugS5YsyXtff1ZB0vwy0n369AFg2LBhrq/QMtv+ovS6Flsn0RNPPAFkl+idPXs2AK1atcr7WFs6/YYbbnB9hV6J9q/S2sXnfulbO2tjS4UDDB06FIj/zJTdymDRokWuz179tVf6ABo1alTegQWwBQLsTA0Elw1Pml/+8peuPX369Jzb7fH+u9/9zvV99NFHrr3bbrsBcP3117u+apixi4qdWTn66KNdny1yYI9/KHxrAxtvyHxP9+rVK5rBVpgtPGEzJGpjC08cccQRW/V6y5YtAzIFggBWr16dc79vfvObW/U6klz2GPFnkYMMGTIEgG984xslHxNkMor8wjWWP1b7t4BmpkRERERERMpMJ1MiIiIiIiIhxDLNz5++ryt96lvf+laph5M4EydOdG2bYuOnnBSakibB/EIKW2rYsKFrb7/99q69atUqIJk7xhfDLkL2UxjrSu+z2rRpk/XfQmzcuBHIpAgCvPPOOwDcfffdrs/uZ3XHHXe4PruvWNzZxbH+59YWnojDe/jjH//o2oMHDwbgsMMOc339+vUr+5iidsIJJ7i2TUV78cUXXZ+f8hvE7m9WzL4p1WTvvfd27Xyp+bZ4TCHs8/jprVdddRUA11xzjeuLQ/prlNasWQNk75kXxBaeaNeuXdGvYdO2AE499VQA/v73v+fc7/TTT3dtW+hDsq1duzan73//+x+Qnbpti9dss03y5jgeeeQRIPiz/b3vfc+1/T3PysEW/CnHcqDk/dRERERERERiQCdTIiIiIiIiIcQyza8YcdhbJS4WLFgAZKrNQWa6Pmia009D+9GPflTi0dUP+++/v2t36tTJtetKA0qyadOmubZNJ/XTeqLmV+QbNGhQzuvZ/ar89IKg+J922mmlGmKk3n///Zy+Aw44oKxjsBXEPv30U9c3a9YsIJPaB5k01p/97Geu76CDDirHEEvKr+b59NNPA9kVqdatW5fzGD9dtUuXLiUcXfzdeuutrm3j98EHHxT9PEHV/Pxqdf6eSvXdPvvsU/Rj7N8L/ndjUHrfiSeeCGT/3VCf9pf68ssvXdvuV2T3RQN48sknXfuhhx7KebytWGur2wHMmzcPSOb+nrZS5gsvvJBzmz1WAHbdddeyjQkyFS+Dlrb4KYeXXHLJVr+WZqZERERERERCqD+XEuqBt99+G8jsTg6ZvTp89iz9vvvuc33+lWTJz14d9a+S2gIIfvGPiy++OOcx/n4Hf/3rX4HsKzdJ5BeBadKkCQAjR450fS1btgRgp512yvs8X3zxBZBdLMX685//7NpvvfWWa997771AdgEKO+PqXyWzxSjatm3r+pIyG+vvs1NOdjYKMnG+/PLLXZ89pv2rfsOHDwey92WqNvbqal2FfGymAMC//vUvAA4//PDSDSzG/D21ZsyYAcDSpUtz7rd8+XLXtjOeL7/8ct7nvvLKKyMYYbz5e+LkKyjhf8fut99+Rb/O6NGjAXjppZdybvOzLsaMGQPA7rvvXvRrJI0/CzVhwgQg+/eRnWmti/+zsYU7rr32WtdXrcXU7B5ykNmrbN999438ddavXw9kZvgAnn/++Vrv72d3bM3+UpZmpkRERERERELQyZSIiIiIiEgIiU/zGzBgAADjx4+v8Egqzy4a9aeg/X0MrO9+97tANFOb9YU/XWxTHPw0n0MOOSTnMUH7pbVv3971HXvssVEPsyL89A+7UH/UqFGub/bs2UD24lNbnGPSpEmuz6ay+Gl8J510EpDZVwmyF4vmW9A6d+5c1168eDGQ/dlIyoLpoPQP+73n7+dU7J5TfhrazJkzXdumZXz44YeuL+h75LjjjgOyU1X847ta2bSzDRs25L2fnyZpj3O/KEUS95OJQvPmzbP+67P78UFmb7ja2LSpaihwUpfNmze79sKFC2u9X7NmzVz7yCOPLOi5/XR/WxghiN2zEupHep/18ccfu3aPHj1ybrcprDadHWDJkiWubYt6+HvVPfjgg5GPs5Jssac999zT9dm9tGyKM2TS/ApNjayLTe2DzH5o/t8U+fgFwqJQP7/NRUREREREtlIyLs3m4V/Jqu9GjBgBwHPPPZf3fvbKfl0FASTDL8e9evXqnNvPP//8gp5nhx12cG2/NH21sDMYkydPdn22+INv/vz5QHbJaTsL5c+YtmrVKvRYbOluyFy16t69e+jnq5SbbroJgClTprg+W2zGL15iZ5xtERC/z2d/Nu+8847rCyo57V9ltFdVTznlFNdnrzLWB34BgJtvvhnInnmyC6q/853vuD7/53X99dcDsPPOO7u+fv36lWawCebPusyZMyfvfc8++2wADj744FIOKVH8bRTsDPPJJ5/s+mxBqp49e7o+v/S5f6Xf6tq1K5BdUKk+8bdAeP311wHYtGmT67O/w/xS9J07d3ZtW2jKFp2oRj/4wQ8A2GuvvVyfnZny2W1L/GwW+zulmIJQtuS5P2vozwbmY39n/vznPy/49QqhmSkREREREZEQdDIlIiIiIiISQizT/Pw0H7vvQW3swvLXXnvN9bVu3bok44qjjRs3urZd1Ofvi2D5df1teoSUn7+zfDXziyEUWxhha73xxhtA9kLUoUOHlnUMUWratCmQ2XcH4MYbbwSyF/fa9+2n7AXtSG/5BVLOOOMM17ZpF/4Cdj8toz7yF0z7e8VZl112GZB9rAelqP7zn/8sweiqh5/2a/dLDEqrhkxasGT4RVGuuOIKAF555RXXd8899wCwcuXKvM/jfx/YtNYGDRpENs4k8d93UNq0tXbtWtf2/x61OnToEO3AYsgvDjVw4EAge99Im0bqFzeySyj8pRRBglLR62JT1f197h5//HEAdtxxx4Keo1CamRIREREREQkhljNT5513nmvbxXtBi9kgc9b75ptvur76NDNlF/JDpvBE0Fm7X5JXto69QuJfKbHl0v2+YcOG5TymWsqhx5ldJHzEEUe4vkILhMRZt27dXNsuovWLSEyYMAEIvoJ3zjnnuL7GjRvnPLc/IyDh+Vdcg+y3335lGkky9enTx7Vrm5GyPvnkk1IPJzb8q+j2M3/ggQfmfYwtBjRo0KC89/NnTMaNGwdkF0pJyhYSleZv6bF06VLXtr+H/II+1cr/PdOrVy8gOxvH/j1fSv3793ftX/ziF0Dw9iJR08yUiIiIiIhICDqZEhERERERCSGW87dHHXWUa7dv3x7ILBqTbI8++mje21u0aAHA+PHjyzGceiEojTIolcK/X/PmzYHq3FsqbubNmwdA27ZtKzyS0rH7lfkpzfUpvTlubIGTdevW5b1fjx49yjGcxJo7d27e2/00abv/UX3g/y454IADgOy/iewC+6uvvtr12YI0Qfx00969e7u2LXYj0WnYsCEA2267bYVHUl42ndz/2/Mf//gHAPfdd1/RzxeUvu4XlhgyZAiQnRK7zTblmy/SzJSIiIiIiEgIsZyZ8t19990ALFu2zPXNnDnTtQ866CCguq9C5/Pqq6/m9NnZKIApU6aUczhVy9/dvEmTJgCsWrUq72MuvPBC177ooosAaNasWQlGJ76xY8cCcOaZZ1Z4JFIt/AXM9sqnXwq5rsITjz32WM7zSPH69u3r2lGXNk4Ke1W+e/fuObfZwjSQmXHyZ7WOP/54ILMwH1RgQkqrUaNGrm2zzvzss2qhmSkREREREZEQdDIlIiIiIiISgvEXdQXIe2M9V9gWzLkU09olIqazZs0CMsVRIJNK4e8lMWnSJNc+9NBDyzO4XGFjCgk9VnfbbTcge2+1yZMnR/0yiThWEyYRMZ0+fToAnTt3dn1fffUVAF26dHF9HTt2dO0LLrgAqEhKVSJiavl70tgCCn4cbdp/hSUqpgmR+Jja4gqQvezEfub9Qgz+90QJJT6mMVRrTDUzJSIiIiIiEoJWHooUyZbn3bx5c4VHIkE6deoEwHvvvVfhkUg16tChAwDr16+v8Eiqz1NPPVXpIYiEYgtTQXZxFFuoZvHixeUekpSRZqZERERERERC0MmUiIiIiIhICCpAEZ4W90VPMY1evStAUSY6VqOnmEZPMY2eYhq9qoqp3eMLYOrUqQAsXLjQ9TVt2rQcw6iqmMaEClCIiIiIiIhESTNT4emsP3qKafQ0M1UaOlajp5hGTzGNnmIaPcU0eopp9DQzJSIiIiIiEiWdTImIiIiIiIRQV5qfiIiIiIiIBNDMlIiIiIiISAg6mRIREREREQlBJ1MiIiIiIiIh6GRKREREREQkBJ1MiYiIiIiIhKCTKRERERERkRD+D9Qx/r/hSmYmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x108 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize = 1.5\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28, 28), cmap=\"gray_r\")\n",
    "    plt.title('Class: {}'.format(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP network definition\n",
    "\n",
    "Let's define the network as a Python class.  We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass.\n",
    "\n",
    "Finally, we define an optimizer to update the model parameters based on the computed gradients.  We select *stochastic gradient descent (with momentum)* as the optimization algorithm, and set *learning rate* to 0.01.  Note that there are [several different options](http://pytorch.org/docs/optim.html#algorithms) for the optimizer in PyTorch that we could use instead of *SGD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=50, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (fc2_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 50)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Let's now define functions to `train()` and `validate()` the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function.  An *epoch* means one pass through the whole training data. After each epoch, we evaluate the model using `validate()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.287841\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.149627\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.369063\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.774798\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.844541\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.667120\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.506478\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.707272\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.468098\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.442599\n",
      "\n",
      "Validation set: Average loss: 0.3445, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.670430\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.612095\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.195346\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.002800\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.167393\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.448279\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.157280\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.378131\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.292422\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.440854\n",
      "\n",
      "Validation set: Average loss: 0.2528, Accuracy: 9271/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.961011\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.285166\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.406207\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.329875\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.283333\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.478841\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.437824\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.606998\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.369896\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.108666\n",
      "\n",
      "Validation set: Average loss: 0.2072, Accuracy: 9399/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.265165\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.340529\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.097147\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.578042\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.167482\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.212131\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.349992\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.340411\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.168108\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.216274\n",
      "\n",
      "Validation set: Average loss: 0.1796, Accuracy: 9469/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.427017\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.208728\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.458668\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.092820\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.069348\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.257746\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.242052\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.225638\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.121117\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.341139\n",
      "\n",
      "Validation set: Average loss: 0.1619, Accuracy: 9526/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.466830\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.249490\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.102461\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.168376\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.349728\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.077708\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.135405\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.421536\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.151573\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.160862\n",
      "\n",
      "Validation set: Average loss: 0.1481, Accuracy: 9553/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.341339\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.167619\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.039526\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.079971\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.109094\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.129461\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.121320\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.330187\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.231141\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.247793\n",
      "\n",
      "Validation set: Average loss: 0.1402, Accuracy: 9576/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.144719\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.236489\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.038001\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.055399\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.225377\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.068672\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.097178\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.026955\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.171098\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.220311\n",
      "\n",
      "Validation set: Average loss: 0.1338, Accuracy: 9600/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.226944\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.108945\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.123464\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.131325\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.128506\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.178690\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.165948\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.237075\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.085619\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.062784\n",
      "\n",
      "Validation set: Average loss: 0.1262, Accuracy: 9617/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.088049\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.442159\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.582061\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.271821\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.286246\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.096990\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.293191\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.078301\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.262693\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.032543\n",
      "\n",
      "Validation set: Average loss: 0.1224, Accuracy: 9627/10000 (96%)\n",
      "\n",
      "CPU times: user 1min 33s, sys: 12.6 s, total: 1min 45s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
